# Paper List of Multi-Modal Dialogue

Papers about multi-modal dialogue, including methods, datasets and related metrics.

We split the task related to multi-modal dialogue as **Visual-Grounded Dialogue (VGD, including Visual QA or VQA), Visual Question Generation, Multimodal Conversation** and **Visual Navigation**.



## Dataset

|                           Dataset                            |               Task               |  Publisher  |       Author        |
| :----------------------------------------------------------: | :------------------------------: | :---------: | :-----------------: |
|  [VQA:  Visual Question Answering](http://cloudcv.org/vqa)   |            visual QA             |  ICCV 2015  |    Virginia Tech    |
|       [Visual  Dialog](https://visualdialog.org/data)        |            visual QA             |  CVPR 2017  |  VisualDialog Org.  |
| [GuessWhat?!  Visual object discovery through multi-modal  dialogue](https://guesswhat.ai/download) |            visual QA             |  CVPR 2017  |   Montreal Univ.    |
| [Visual  Reference Resolution using Attention Memory for Visual  Dialog](http://cvlab.postech.ac.kr/research/attmem) |            visual QA             |  NIPS 2017  |   Postech&Disney    |
| [CLEVR:  A diagnostic dataset for compositional language and elementary visual  reasoning](https://cs.stanford.edu/people/jcjohns/clevr/) |            visual QA             |  CVPR 2017  |      Stanford       |
| [Image-grounded  conversations: Multimodal context for natural question and response  generation](https://www.microsoft.com/en-us/download/details.aspx?id=55324) |            visual QA             | IJCNLP 2017 | Rochester&Microsoft |
| [Towards  Building Large Scale Multimodal Domain-Aware Conversation Systems  (MMD)](https://amritasaha1812.github.io/MMD/) |         multimodal conv          |  AAAI 2018  |         IBM         |
| [Talk  the walk: Navigating new york city through grounded  dialogue](https://github.com/facebookresearch/talkthewalk) |        visual navigation         |  ICLR 2019  |        MILA         |
|      [Vision-and-Dialog  Navigation](https://cvdn.dev/)      |        visual navigation         |  CoRL 2019  |         UoW         |
| [CLEVR-Dialog:  A Diagnostic Dataset for Multi-Round Reasoning in Visual  Dialog](https://github.com/satwikkottur/clevr-dialog) |          visual-dialog           | NAACL 2019  |         CMU         |
| [Image-Chat:  Engaging Grounded Conversations](http://parl.ai/projects/image_chat) |          visual dialog           |   ACL2020   |      Facebook       |
|    [OpenViDial](https://github.com/ShannonAI/OpenViDial)     |         visual-sentence          | arxiv 2020  |      ShannonAI      |
| [Situated  and Interactive Multimodal Conversations  (SIMMC)](https://github.com/facebookresearch/simmc) |  multimodal conv /  navigation   | COLING 2020 |      Facebook       |
| [PhotoChat:  A human-human dialogue dataset with photo sharing behavior for joint  image-text  modeling](https://github.com/google-research/google-research/tree/master/multimodalchat/) | multimodal  conversation/sharing |  ACL 2021   |       Google        |
| [MMConv:  An Environment for Multimodal Conversational Search across Multiple  Domains](https://github.com/lizi- git/MMConv) |         multimodal conv          | SIGIR 2021  |         NUS         |
| [Constructing  Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant  Images](https://github.com/shh1574/ multi-modal-dialogue-dataset) |     multimodal conversation      |   ACL2021   |        KAIST        |
| [OpenViDial  2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual  Contexts](https://github.com/ShannonAI/OpenViDial) |         visual-sentence          | arxiv 2021  |      ShannonAI      |
| [MMChat:  Multi-Modal Chat Dataset on Social Media](https://  github.com/silverriver/MMChat) |          visual dialog           |  LREC 2022  |       Alibaba       |
| [MSCTD:  A Multimodal Sentiment Chat Translation  Dataset](https://github.com/XL2248/MSCTD) |          visual dialog           | arxiv 2022  |       Tencent       |



## Methods

### Visual Grounded Dialogue

Visual grounded dialogue considers only **one image** for one dialogue session. The whole session is constrained to this given image. It is also know as *Visual Dialog* task.

We roughly split the learning paradigm of different methods as: *Fusion-Based (FB), Attention-Based (AB) and Reinforce Learning (RL)*.

|                       Title                       | Dataset Used | Publisher | Code | Class |
| :-----------------------------------------------: | :----------: | :-------: | :--: | :---: |
| [Visual Dialog](https://arxiv.org/abs/1611.08669) |              | ICCV2017  | N/A  |  FB   |
|                                                   |              |           |      |       |
|                                                   |              |           |      |       |
|                                                   |              |           |      |       |
|                                                   |              |           |      |       |
|                                                   |              |           |      |       |
|                                                   |              |           |      |       |

